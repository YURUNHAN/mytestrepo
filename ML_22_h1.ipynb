{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_22_h1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Z3f_sttGtY"
      },
      "source": [
        "# **Homework Assignment #1**\n",
        "\n",
        "Assigned: January 10, 2022\n",
        "\n",
        "Due: January 24, 2022\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "You must complete this assignment individually; you are not allowed to collaborate with anyone\n",
        "else. You may discuss the homework to understand the problems and the mathematics behind the methods, but you are not allowed to share problem solutions or your code with any other students. Similarly, you are also not allowed to consult the internet for solutions.\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(15 points) Consider a scenario in which you train a decision tree to classify credit card transactions into fraudulent (+) or legitimate (-). You have a dataset with 10,000 labeled transactions (5,000 + and 5,000 -). You randomly select 8,000 examples to build a decision tree with a minimum leaf size of 2 data points, then test your tree on the held-out 2,000 data points. You observe that your training error is 0.01 but your test error is 0.20. Explain why this might happen and the steps that you could take to improve the performance of the decision tree algorithm.\n",
        "\n",
        "#2.\n",
        "\n",
        "(25 points) Supreme Leader Snoke of the First Order has been concerned about the number of stormtroopers who have displayed aberrant behavior such as dereliction of duty on the battlefield lately, so Kylo Ren has asked Captain Phasma to build a decision tree to predict whether behavior is an Abberation (the target variable) based on the features Base (Starkiller or Snoke), Clone (Yes or No), Veteran (Yes or No), and Commander (Phasma, Inky, Blinky, or Clyde). Use the Information Gain criteria described in class to build a decision tree based on the training data below. Show each calculation you perform to generate the tree. You may upload the solution as a tree picture or describe the nodes and edges at each level of the tree.\n",
        "\n",
        "ID | Base | Clone | Veteran | Commander | Aberration\n",
        "--- | --- | --- | --- | --- | ---\n",
        "1 | Starkiller | No | No | Phasma | No\n",
        "2 | Starkiller | No | No | Phasma | No\n",
        "3 | Starkiller | No | Yes | Inky | No\n",
        "4 | Snoke | No | Yes | Inky | Yes\n",
        "5 | Snoke | No | No | Phasma | Yes\n",
        "6 | Snoke | Yes | Yes | Inky | No\n",
        "7 | Starkiller | Yes | No | Blinky | No\n",
        "8 | Starkiller | Yes | Yes | Blinky |Yes\n",
        "9 | Starkiller | Yes | Yes | Blinky | Yes\n",
        "10 | Starkiller | Yes | No | Blinky | No\n",
        "11 | Snoke | No | Yes | Clyde | No\n",
        "12 | Snoke | Yes | No | Clyde | No\n",
        "13 | Snoke | No | Yes | Clyde | No\n",
        "14 | Snoke | Yes | Yes | Clyde | No\n",
        "\n",
        "#3.\n",
        "\n",
        "(10 points) Express the concept (Aberration=Yes) learned by your trees in Problem 1 as logical if-then rules.\n",
        "\n",
        "#4.\n",
        "\n",
        "(10 points) Suppose you are testing a new algorithm on a data set consisting of 100 positive and 100 negative examples. You plan to use leave-one-out cross-validation and compare your algorithm to a baseline function, a simple majority classifier. With leave-one-out cross-validation, you train the algorithm on 199 data points and test it on 1 data point. You repeat the process 200 times, letting each point having a chance to represent the test set, and report the average of the classification accuracies. Given a set of training data, the majority classifier always outputs the class that is in the majority in the training set, regardless of the input. You expect the majority classifier to achieve about 50% classification accuracy, but to your surprise, it scores zero every time. Why?\n",
        "\n",
        "#5.\n",
        "\n",
        "(60 points) In this problem you are asked to write a Python program using Google Colab. We provide code below to construct a decision tree using the measures of entropy and gain discussed in class. You need to add three steps.\n",
        "\n",
        "- First, implement a majority classifier as we described in class and update the code to use either the majority classifier or the decision tree on the dataset.\n",
        "\n",
        "- Second, implement a \"random choice\" classifier that picks a class label based on a uniform probability distribution and update the code to use any of the three alternative classifiers.\n",
        "\n",
        "- Third, update the code to run each classifier 10 times and average the accuracy for each classifier over the 10 runs. Each run should randomly pick 7 training data points from the set and use the remaining 3 points for classification. Report the average accuracies for your three methods.\n",
        "\n",
        "*Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc8fTGIkJcim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37092524-3d06-4039-9c02-8795056a3d3f"
      },
      "source": [
        "# Decision tree learning\n",
        "#\n",
        "# Assumes discrete features. Examples may be inconsistent. Stopping condition for tree\n",
        "# generation is when all examples have the same class, or there are no more features\n",
        "# to split on (in which case, use the majority class). If a split yields no examples\n",
        "# for a particular feature value, then the classification is based on the parent's\n",
        "# majority class.\n",
        "\n",
        "import math\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, majClass):\n",
        "        self.split_feature = -1 # -1 indicates leaf node\n",
        "        self.children = {} # dictionary of {feature_value: child_tree_node}\n",
        "        self.majority_class = majClass\n",
        "        \n",
        "def build_tree(examples):\n",
        "    if not examples:\n",
        "        return None\n",
        "    # collect sets of values for each feature index, based on the examples\n",
        "    features = {}\n",
        "    for feature_index in range(len(examples[0]) - 1):\n",
        "        features[feature_index] = set([example[feature_index] for example in examples])\n",
        "    return build_tree_1(examples, features)\n",
        "    \n",
        "def build_tree_1(examples, features):\n",
        "    tree_node = TreeNode(majority_class(examples))\n",
        "    # if examples all have same class, then return leaf node predicting this class\n",
        "    if same_class(examples):\n",
        "        return tree_node\n",
        "    # if no more features to split on, then return leaf node predicting majority class\n",
        "    if not features:\n",
        "        return tree_node\n",
        "    # split on best feature and recursively generate children\n",
        "    best_feature_index = best_feature(features, examples)\n",
        "    tree_node.split_feature = best_feature_index\n",
        "    remaining_features = features.copy()\n",
        "    remaining_features.pop(best_feature_index)\n",
        "    for feature_value in features[best_feature_index]:\n",
        "        split_examples = filter_examples(examples, best_feature_index, feature_value)\n",
        "        tree_node.children[feature_value] = build_tree_1(split_examples, remaining_features)\n",
        "    return tree_node\n",
        "\n",
        "def majority_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return max(set(classes), key = classes.count)\n",
        "\n",
        "def same_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return (len(set(classes)) == 1)\n",
        "\n",
        "def best_feature(features, examples):\n",
        "    # Return index of feature with lowest entropy after split\n",
        "    best_feature_index = -1\n",
        "    best_entropy = 2.0 # max entropy = 1.0\n",
        "    for feature_index in features:\n",
        "        se = split_entropy(feature_index, features, examples)\n",
        "        if se < best_entropy:\n",
        "            best_entropy = se\n",
        "            best_feature_index = feature_index\n",
        "    return best_feature_index\n",
        "\n",
        "def split_entropy(feature_index, features, examples):\n",
        "    # Return weighted sum of entropy of each subset of examples by feature value.\n",
        "    se = 0.0\n",
        "    for feature_value in features[feature_index]:\n",
        "        split_examples = filter_examples(examples, feature_index, feature_value)\n",
        "        se += (float(len(split_examples)) / float(len(examples))) * entropy(split_examples)\n",
        "    return se\n",
        "\n",
        "def entropy(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    classes_set = set(classes)\n",
        "    class_counts = [classes.count(c) for c in classes_set]\n",
        "    e = 0.0\n",
        "    class_sum = sum(class_counts)\n",
        "    for class_count in class_counts:\n",
        "        if class_count > 0:\n",
        "            class_frac = float(class_count) / float(class_sum)\n",
        "            e += (-1.0)* class_frac * math.log(class_frac, 2.0)\n",
        "    return e\n",
        "\n",
        "def filter_examples(examples, feature_index, feature_value):\n",
        "    # Return subset of examples with given value for given feature index.\n",
        "    return list(filter(lambda example: example[feature_index] == feature_value, examples))\n",
        "\n",
        "def print_tree(tree_node, feature_names, depth = 1):\n",
        "    indent_space = depth * \"  \"\n",
        "    if tree_node.split_feature == -1: # leaf node\n",
        "        print(indent_space + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "    else:\n",
        "        for feature_value in tree_node.children:\n",
        "            print(indent_space + feature_names[tree_node.split_feature] + \" == \" + feature_value)\n",
        "            child_node = tree_node.children[feature_value]\n",
        "            if child_node:\n",
        "                print_tree(child_node, feature_names, depth+1)\n",
        "            else:\n",
        "                # no child node for this value, so use majority class of parent (tree_node)\n",
        "                print(indent_space + \"  \" + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "\n",
        "def classify(tree_node, instance):\n",
        "    if tree_node.split_feature == -1:\n",
        "        return tree_node.majority_class\n",
        "    child_node = tree_node.children[instance[tree_node.split_feature]]\n",
        "    if child_node:\n",
        "        return classify(child_node, instance)\n",
        "    else:\n",
        "        return tree_node.majority_class\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   feature_names = [\"Color\", \"Type\", \"Origin\", \"Stolen\"]\n",
        "   \n",
        "   examples = [\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Yellow\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Yellow\", \"Sports\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Imported\", \"Yes\"]\n",
        "       ]\n",
        "   tree = build_tree(examples)\n",
        "   print(\"Tree:\")\n",
        "   #print_tree(tree, feature_names)\n",
        "   test_instance = [\"Red\", \"SUV\", \"Domestic\"]\n",
        "   test_class = classify(tree, test_instance)\n",
        "   print(\"\\nTest instance: \" + str(test_instance))\n",
        "   print(\"  class = \" + test_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tree:\n",
            "  Type == SUV\n",
            "    Color == Red\n",
            "      Stolen: No\n",
            "    Color == Yellow\n",
            "      Origin == Domestic\n",
            "        Stolen: No\n",
            "      Origin == Imported\n",
            "        Stolen: Yes\n",
            "  Type == Sports\n",
            "    Origin == Domestic\n",
            "      Color == Red\n",
            "        Stolen: Yes\n",
            "      Color == Yellow\n",
            "        Stolen: No\n",
            "    Origin == Imported\n",
            "      Stolen: Yes\n",
            "\n",
            "Test instance: ['Red', 'SUV', 'Domestic']\n",
            "  class = No\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}